{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><B><left>Hello, This Notebook Contains Example of Corona Virus Tweets Multi Class Classification.</left></B></h2>\n",
    " <h4><left>- Classes is: Extremely Positive, Positive, Extremely Negative, Negative. </left></h4>\n",
    "<h4><I><left>- We do tweets cleaning, tokenization, normalization, also get the binary classes.</left></I></h4>\n",
    "<h4><I><left>- Try to generate tweets from data.</left></I></h4>\n",
    "<h4><I><left>- Vectorizing texts usiing different vectorization way: BOW, TF-IDF and finally Word Embedding.</left></I></h4>\n",
    "<h4><I><left>- Train different Models(Classifiers) from Scikit-Learn library such as: SVM, Logistic Regression and MulinomialNB for classification.</left></I></h4>\n",
    "<h4><I><left>- Train Deep Neural Networks Models with Tensorflow and Keras such as: LSTM, BiLSTM, CNN and FC for both multi class classification and binary classification.</left></I></h4>\n",
    "\n",
    "<h4><I><left>- Finally we compare results and save the best Model.</left></I></h4>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "            <li><h3>Contents:</h3></li>\n",
    "   <ul>\n",
    "        <li><h4><a href=\"#import\">Import Libraries and Helper Functions.</a></h4></li>\n",
    "        <li><h4><a href=\"#prep\">Prepocessing.</a></h4></li>\n",
    "        <li><h4><a href=\"#generate\">Create Tweets Generator.</a></h4></li>\n",
    "        <li><h4><a href=\"#machine\">Modeling Using Machine-Learning Models.</a></h4></li>\n",
    "         <ul>\n",
    "            <li><a href=\"#bow_cls\">classification with BOW vectors.</a></li>\n",
    "            <li><a href=\"#tfidf_cls\">classification with TF-IDF vectors.</a></li>\n",
    "        </ul>\n",
    "        <li><h4><a href=\"#deep_nn\">Modeling Using Deep Neural Network Models.</a></h4></li>\n",
    "         <ul>\n",
    "            <li><a href=\"#feature_ext\">feature extraction.</a></li>\n",
    "            <li><a href=\"#multi_nn\">multi class classification.</a></li>\n",
    "            <li><a href=\"#binary_nn\">binary class classification</a></li>\n",
    "        </ul>  \n",
    "       <li><h4><a href=\"#save\">Save The Best Model.</a></h4></li>\n",
    "    </ul>\n",
    "    </ul>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "<h3><B><left>- Import Data and Helper Functions: </left></B></h3>\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs:\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import re \n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from nltk.lm import MLE\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_labels(sentiment):\n",
    "    if sentiment == \"Positive\":\n",
    "        return 'pos'\n",
    "    elif sentiment == \"Extremely Positive\":\n",
    "        return 'pos'\n",
    "    elif sentiment == \"Extremely Negative\":\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Applications\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "my_words = ['against','after','through','above','to','over',\n",
    "           'few','no','nor','not','only','to','can','cant','will','just','dont','should','shouldnt'\n",
    "           'now','couldnt','didnt','hasnt','havent','isnt','arent','should','shouldnt','need','neednt','was','wasnt','were',\n",
    "            'werent']\n",
    "final_stop_words = [word for word in stop_words if not word in my_words]\n",
    "\n",
    "# Reading Dataset:{Two ways: binary labels and all labels} \n",
    "data_frame = pd.read_csv('E:\\Books and Resources\\Educational\\Covid_19_tweets_train.csv',encoding='latin1')\n",
    "labels = data_frame['Sentiment']\n",
    "tweets = data_frame['OriginalTweet']\n",
    "\n",
    "\n",
    "data_frame_binary = data_frame[data_frame.Sentiment != \"Neutral\"]\n",
    "data_frame_binary[\"Sentiment\"] = data_frame_binary[\"Sentiment\"].apply(get_binary_labels)\n",
    "binary_tweets = data_frame_binary['OriginalTweet']\n",
    "binary_labels = data_frame_binary['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_cleaning(tweets):\n",
    "    url_ptn = re.compile(r'http(s?)\\S+')\n",
    "    ptn = re.compile(r'[^A-Za-z _#@]')\n",
    "    ptn2 = re.compile(r'(https\\w+ ?)|(@\\S+)|(#\\S+)|# |@ |_')\n",
    "    ptn3 = re.compile(r'([\\w])\\1+')\n",
    "    new_tweets = []\n",
    "    tweets = [tweet.lower() for tweet in tweets]\n",
    "    \n",
    "    tweets = [re.sub(url_ptn,'',tweet) for tweet in tweets]\n",
    "    \n",
    "    tweets = [re.sub(ptn,' ',tweet) for tweet in tweets]\n",
    "    \n",
    "    tweets = [re.sub(ptn2,'',tweet) for tweet in tweets]\n",
    "    \n",
    "    tweets_tokens = [[word for word in tweet.split() if not word in final_stop_words ] for tweet in tweets]\n",
    "    for tweet in tweets_tokens:\n",
    "        new_tweets.append(' '.join([word for word in tweet]))\n",
    "        \n",
    "    tweets = new_tweets\n",
    "    \n",
    "    tweets =[re.sub(ptn3,r'\\1',tweet) for tweet in tweets]\n",
    "    \n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    tweets = [\" \".join([wnl.lemmatize(word) for word in tweet.split() ])for tweet in tweets]\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def testing_result(path):\n",
    "    data_frame = pd.read_csv(path,encoding='latin1')\n",
    "    \n",
    "    #get data & labels\n",
    "    tweets = data_frame['OriginalTweet']\n",
    "    labels = data_frame['Sentiment']\n",
    "    #cleaning\n",
    "    cleaned_tweets = result_cleaning(tweets)\n",
    "\n",
    "    # tokenization: \n",
    "    tweets_tokens = get_tokens(tweets)\n",
    "    clean_tokens = get_tokens(cleaned_tweets)\n",
    "    \n",
    "    #one_hot_labels\n",
    "    one_hot_en = OneHotEncoder(sparse=False)\n",
    "    one_hot_labels = one_hot_en.fit_transform(labels.values.reshape(-1,1))\n",
    "    \n",
    "    #convert to sequences and padding:\n",
    "    maxlen = 50\n",
    "    X_test = tokenizer.texts_to_sequences(labels)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "    model = load_model(\"model_5_classes.h5\")\n",
    "    model.load_weights(\"model_5_classes_w.hdf5\")\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, one_hot_labels, verbose=False)\n",
    "    print(\" accuracy on test is:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(tweets):\n",
    "    tweets_tokens = [[word for word in tweet.split() if not word in final_stop_words] for tweet in tweets]\n",
    "    return tweets_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed):\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "<h3><left>- Data Preprocessing: </left></h3>\n",
    "\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data:\n",
    "cleaned_tweets = result_cleaning(tweets)\n",
    "cleaned_binary_tweets = result_cleaning(binary_tweets)\n",
    "\n",
    "# tokenization: \n",
    "tweets_tokens = get_tokens(tweets)\n",
    "clean_tokens = get_tokens(cleaned_tweets)\n",
    "\n",
    "# data splitting:\n",
    "sentences_train_b, sentences_test_b, y_train_b, y_test_b = train_test_split(cleaned_binary_tweets, binary_labels, test_size=0.25, random_state=1000)\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(cleaned_tweets, labels, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generate\"></a>\n",
    "<h3><left>- Tweets Generation: </left></h3>\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Padded Sentence:\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'man home bakery instead supermarket not provided adequate guidance to consumer transportation warehousing agri market'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating Text Generation:\n",
    "model_mle = MLE(n)\n",
    "model_mle.fit(train_data, padded_sents)\n",
    "\n",
    "generate_sent(model_mle,15,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"machine\"></a>\n",
    "<h3><B><left>- Modeling with Machine-Learning Classifiers:</left></B></h3>\n",
    "<h4><I><left>- classification using BOW victors.</left></I></h4>\n",
    "<h4><I><left>- classification using TF-IDF victors.</left></I></h4>\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bow_cls\"></a>\n",
    "<h4><B><left>- classification using BOW victors:</left></B></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Create BOW:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(sentences_train)\n",
    "vectorizer.transform(sentences_train).toarray()\n",
    "\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test  = vectorizer.transform(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC Classifier: \n",
    "from sklearn.svm import SVC\n",
    "classifier_svc = SVC()\n",
    "classifier_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVC: 0.6104\n"
     ]
    }
   ],
   "source": [
    "# SVC Score:\n",
    "score = classifier_svc.score(X_test, y_test)\n",
    "print('Accuracy for SVC: {:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NuSVC()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NuSVC Classifier:\n",
    "from sklearn.svm import NuSVC\n",
    "classifier_nu = NuSVC()\n",
    "classifier_nu.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for NuSVC: 0.6357\n"
     ]
    }
   ],
   "source": [
    "# NuSVC Score:\n",
    "score = classifier_nu.score(X_test, y_test)\n",
    "print('Accuracy for NuSVC: {:.4f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tfidf_cls\"></a>\n",
    "<h4><B><left>- classification using TF-IDF victors:</left></B></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB ,BernoulliNB\n",
    "from sklearn.svm import NuSVC,SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# tfid vectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "\n",
    "# classifiers\n",
    "nuSVC = NuSVC()\n",
    "mnb = MultinomialNB()\n",
    "lr = LogisticRegression(solver = \"lbfgs\")\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "# model piplines\n",
    "model_nu = Pipeline([('vectorizer',tvec),('classifier',nuSVC)])\n",
    "model_mnb = Pipeline([('vectorizer',tvec),('classifier',mnb)])\n",
    "model_lr = Pipeline([('vectorizer',tvec),('classifier',lr)])\n",
    "model_sgd = Pipeline([('vectorizer',tvec),('classifier',sgd)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NuSvc Model:\n",
    "model_nu.fit(sentences_train, y_train)\n",
    "predictions_nu = model_nu.predict(sentences_test)\n",
    "confusion_matrix(predictions_nu, y_test)\n",
    "\n",
    "print(\"Accuracy Tf-Idf NuSVC: {:.4f}\".format(accuracy_score(predictions_nu, y_test)))\n",
    "print(\"Precision Tf-Idf NuSVC: {:.4f}\".format(precision_score(predictions_nu, y_test, average = 'weighted')))\n",
    "print(\"Recall Tf-Idf NuSVC: {:.4f}\".format(recall_score(predictions_nu, y_test, average = 'weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Tf-Idf MNB : 0.3995\n",
      "Precision Tf-Idf MNB: 0.7304\n",
      "Recall Tf-Idf MNB: 0.3995\n"
     ]
    }
   ],
   "source": [
    "#MNB Model:\n",
    "model_mnb.fit(sentences_train, y_train)\n",
    "predictions_mnb = model_mnb.predict(sentences_test)\n",
    "confusion_matrix(predictions_mnb, y_test)\n",
    "\n",
    "print(\"Accuracy Tf-Idf MNB : {:.4f}\".format(accuracy_score(predictions_mnb, y_test)))\n",
    "print(\"Precision Tf-Idf MNB: {:.4f}\".format(precision_score(predictions_mnb, y_test, average = 'weighted')))\n",
    "print(\"Recall Tf-Idf MNB: {:.4f}\".format(recall_score(predictions_mnb, y_test, average = 'weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Tf-Idf LR : 0.5944\n",
      "Precision Tf-Idf LR: 0.6013\n",
      "Recall Tf-Idf LR: 0.5944\n"
     ]
    }
   ],
   "source": [
    "#LR Model:\n",
    "model_lr.fit(sentences_train, y_train)\n",
    "predictions_lr = model_lr.predict(sentences_test)\n",
    "confusion_matrix(predictions_lr, y_test)\n",
    "\n",
    "print(\"Accuracy Tf-Idf LR : {:.4f}\".format(accuracy_score(predictions_lr, y_test)))\n",
    "print(\"Precision Tf-Idf LR: {:.4f}\".format( precision_score(predictions_lr, y_test, average = 'weighted')))\n",
    "print(\"Recall Tf-Idf LR: {:.4f}\".format(recall_score(predictions_lr, y_test, average = 'weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Tf-Idf SGD : 0.5580\n",
      "Precision Tf-Idf SGD: 0.6472\n",
      "Recall Tf-Idf SGD: 0.5580\n"
     ]
    }
   ],
   "source": [
    "#SGD Model:\n",
    "model_sgd.fit(sentences_train, y_train)\n",
    "predictions_sgd = model_sgd.predict(sentences_test)\n",
    "confusion_matrix(predictions_sgd, y_test)\n",
    "\n",
    "print(\"Accuracy Tf-Idf SGD : {:.4f}\".format(accuracy_score(predictions_sgd, y_test)))\n",
    "print(\"Precision Tf-Idf SGD: {:.4f}\".format(precision_score(predictions_sgd, y_test, average = 'weighted')))\n",
    "print(\"Recall Tf-Idf SGD: {:.4f}\".format(recall_score(predictions_sgd, y_test, average = 'weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deep_nn\"></a>\n",
    "<h3><B><left>- Modeling using Deep Neural Networks:</left></B></h3>\n",
    "<h4><I><left>- exteact sequences for feed into network.</left></I></h4>\n",
    "<h4><I><left>- build Model for multi class classification with Embeddings Layer(to convert words into vectors).</left></I></h4>\n",
    "<h4><I><left>- build Model for binary class classification with Embeddings Layer(to convert words into vectors).</left></I></h4>\n",
    "\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature_ext\"></a>\n",
    "<h4><B><left>- extract sequences for feed to network: </left></B></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels_encoded = encoder.fit_transform(labels)\n",
    "\n",
    "one_hot_en = OneHotEncoder(sparse=False)\n",
    "one_hot_labels = one_hot_en.fit_transform(y_train.values.reshape(-1,1))\n",
    "one_hot_labels_test = one_hot_en.fit_transform(y_test.values.reshape(-1,1))\n",
    "\n",
    "one_hot_labels_b = one_hot_en.fit_transform(y_train_b.values.reshape(-1,1))\n",
    "one_hot_labels_test_b = one_hot_en.fit_transform(y_test_b.values.reshape(-1,1))\n",
    "\n",
    "# Tokenization:\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "tokenizer2 = Tokenizer(num_words=10000)\n",
    "tokenizer2.fit_on_texts(sentences_train_b)\n",
    "\n",
    "\n",
    "# Text to Sequence:\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "X_train_b = tokenizer2.texts_to_sequences(sentences_train_b)\n",
    "X_test_b = tokenizer2.texts_to_sequences(sentences_test_b)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "vocab_size_b = len(tokenizer2.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "\n",
    "# Padding:\n",
    "maxlen = 50\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "                                         \n",
    "X_train_b = pad_sequences(X_train_b, padding='post', maxlen=maxlen)\n",
    "X_test_b = pad_sequences(X_test_b, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"multi_nn\"></a>\n",
    "<h4><B><left>- build Model for multi class classification:</left></B></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           2513600   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 48, 128)           38528     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 48, 256)           263168    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                12850     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 2,829,271\n",
      "Trainable params: 2,829,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#NN for 5 Classes:\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "model_77 = Sequential()\n",
    "model_77.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "\n",
    "\n",
    "model_77.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "\n",
    "model_77.add(layers.Bidirectional(layers.LSTM(units=128,return_sequences=True)))\n",
    "model_77.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "model_77.add(layers.Dense(50, activation='tanh'))\n",
    "model_77.add(layers.Dropout(0.3))\n",
    "model_77.add(layers.Dense(20, activation='relu'))\n",
    "\n",
    "model_77.add(layers.Dense(5,activation = 'softmax'))\n",
    "model_77.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_77.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Applications\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 24693 samples, validate on 6174 samples\n",
      "Epoch 1/3\n",
      "24693/24693 [==============================] - 56s 2ms/step - loss: 1.2637 - accuracy: 0.4511 - val_loss: 0.8589 - val_accuracy: 0.6733\n",
      "Epoch 2/3\n",
      "24693/24693 [==============================] - 40s 2ms/step - loss: 0.6872 - accuracy: 0.7512 - val_loss: 0.6991 - val_accuracy: 0.7462\n",
      "Epoch 3/3\n",
      "24693/24693 [==============================] - 40s 2ms/step - loss: 0.5135 - accuracy: 0.8303 - val_loss: 0.6804 - val_accuracy: 0.7527\n"
     ]
    }
   ],
   "source": [
    "history = model_77.fit(X_train, one_hot_labels,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8573\n",
      "Testing Accuracy:  0.7707\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_77.evaluate(X_train, one_hot_labels, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_77.evaluate(X_test, one_hot_labels_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"binary_nn\"></a>\n",
    "<h4><B><left>- build Model for binary class classification:</left></B></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           2299500   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 48, 128)           38528     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 48, 256)           263168    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                12850     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 2,615,108\n",
      "Trainable params: 2,615,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#NN for 2 Classes:\n",
    "embedding_dim = 100\n",
    "model_77_b = Sequential()\n",
    "model_77_b.add(layers.Embedding(vocab_size_b, embedding_dim, input_length=maxlen))\n",
    "\n",
    "\n",
    "model_77_b.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "\n",
    "model_77_b.add(layers.Bidirectional(layers.LSTM(units=128,return_sequences=True)))\n",
    "model_77_b.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "model_77_b.add(layers.Dense(50, activation='tanh'))\n",
    "model_77_b.add(layers.Dropout(0.3))\n",
    "model_77_b.add(layers.Dense(20, activation='relu'))\n",
    "\n",
    "model_77_b.add(layers.Dense(2,activation = 'softmax'))\n",
    "model_77_b.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_77_b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20066 samples, validate on 5017 samples\n",
      "Epoch 1/3\n",
      "20066/20066 [==============================] - 36s 2ms/step - loss: 0.5063 - accuracy: 0.7313 - val_loss: 0.3317 - val_accuracy: 0.8597\n",
      "Epoch 2/3\n",
      "20066/20066 [==============================] - 33s 2ms/step - loss: 0.2310 - accuracy: 0.9110 - val_loss: 0.2895 - val_accuracy: 0.8782\n",
      "Epoch 3/3\n",
      "20066/20066 [==============================] - 32s 2ms/step - loss: 0.1294 - accuracy: 0.9559 - val_loss: 0.3319 - val_accuracy: 0.8798\n"
     ]
    }
   ],
   "source": [
    "history = model_77_b.fit(X_train_b, one_hot_labels_b,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9614\n",
      "Testing Accuracy:  0.8731\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_77_b.evaluate(X_train_b, one_hot_labels_b, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_77_b.evaluate(X_test_b, one_hot_labels_test_b, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "<h3><B><left>- Save the best model</left></B></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5- Saving Models\n",
    "model_77.save(\"model_5_classes.h5\")\n",
    "model_77.save_weights(\"model_5_classes_w.hdf5\")\n",
    "\n",
    "model_77_b.save(\"model_2_classes.h5\")\n",
    "model_77_b.save_weights(\"model_2_classes_w.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><I><center> ...The End...</center></I></h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
